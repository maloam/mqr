<html>
<body>
<p>T E C H N I C A L _  A L I G N M E N T  _ I N _  A I </p>
Making language models bigger does not inherently make them <i>better</i> at following a user’s intent. </br>
For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. </br>
In other words, these models are not <i>aligned</i> with their users. </br>
<a href="https://maloam.github.io/mqr/tlmtfiwhf/IIIon.html">Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.</a>
</br>
</br>
We have trained language assistants that are both <i>helpful</i> and <i>harmless</i> without using human feedback labels for harmlessness. </br>
We referred to the technique as ‘constitutional AI’ (CAI) since we used a ‘constitution’ consisting of human-written principles. </br>
[...]</br>
We used this method to train models that are both <i>harmless</i> and <i>non-evasive</i>, partially resolving an issue in [Bai et al., 2022].</br>
[...]</br>
Our ultimate goal is <i>not</i> to remove human supervision entirely,... </br>
<a href="https://maloam.github.io/mqr/cahfaF/IIIIIIon.html">Ouyang, Long, et al. "Bai, Yuntao, et al. "Constitutional ai: Harmlessness from ai feedback." arXiv preprint arXiv:2212.08073 (2022).</a>
</body>
</html>
